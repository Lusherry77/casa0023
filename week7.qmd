# Classification I

## Summary

### Why classification matters

Classification helps convert pixel values from satellite images into meaningful land cover types (e.g. vegetation, built-up, water). This is essential for monitoring land change, urban sprawl, and ecosystem health. One real-world example mentioned in class is the use of classification by Brazilian authorities to detect illegal logging in near-real time.

### Classification types

Supervised classification: Requires labelled training data (e.g. CART, Random Forest, SVM) 

Unsupervised classification: No training labels, uses clustering (e.g. K-means, ISODATA) 

CART and SVM can be more accurate, but depend heavily on good sample data. 

K-means is fast but hard to interpret.

### How it works in practice

In the GEE practical, we: 

Collected training data (water, urban, vegetation) 

Used .train() to create a CART classifier 

Applied .classify() to generate a land cover map 

Used a confusion matrix to assess classification accuracy

### Common issues

Overfitting: Small sample sizes can lead to models that don’t generalise well 

Mixed pixels: Especially common in cities (e.g. one pixel contains both road and vegetation) 

“Black box” models: Methods like Random Forest or SVM may be hard to interpret

This week made me realise that classification isn’t just technical—it’s a decision-making process. For example, how we define categories, choose samples, or handle spectral confusion can all affect what our map shows.

## Applications

### Supervised classification in practice

In the GEE practical, we used supervised classification with CART. We collected training data for classes like vegetation, urban, and water, trained the model using .train(), then applied .classify() and assessed results with a confusion matrix.

This matched the supervised workflow described in @Jensen2015, where classification depends heavily on representative training data. Even small spatial bias in our samples—like too few water points—led to frequent misclassification.

### Comparing classifiers: CART, SVM, Random Forest

CART is easy to implement, but prone to overfitting. Random Forest addresses this by aggregating many trees with bootstrapped samples. SVMs are more precise in defining boundaries but require careful tuning.

In urban contexts, these methods have been applied to identify informal settlements, vegetation fragmentation, and impervious surfaces [@GISGeography2014; @Pal2005].

### Accuracy isn’t just a number

The confusion matrix gives a surface-level view of accuracy—but it’s highly sensitive to sample size, class imbalance, and label quality. I realised that changing just a few training points can “improve” accuracy artificially.

This connects to [@Barsi2018], who argue that thematic accuracy is multidimensional, involving sampling, classification logic, and even post-processing.

### Beyond pixels: can classification be smarter?

Unsupervised methods like K-means are useful in data-scarce settings, but hard to interpret. @Jensen2015 also describes “expert systems” that embed domain logic into classification—suggesting potential for hybrid models.

This made me think: can we move beyond purely spectral classification, especially in cities where land cover types are defined by social and spatial context?

## Reflection

This week made me think about classification in a different way. Before, I thought it was a technical step—pick a classifier, label some training points, press run. But when I actually started working with the samples, I realised it’s not that simple. Just deciding where to draw a polygon, or how many points to add, can totally change the outcome. The map still looks clean—but what’s underneath might not be that solid.

It also made me more aware of how “accuracy” can be a bit misleading. It’s easy to get a high overall score, but if your samples are unbalanced or your categories vague, it doesn’t mean the result is useful. That’s something I hadn’t really thought about before. Now I feel like classification isn’t just about getting a nice-looking map—it’s about making conscious decisions, and knowing what your choices might hide or distort.

I still think these tools are powerful, especially in big-scale urban monitoring. But I’m starting to pay more attention to the invisible stuff: what gets included, what gets left out, and how much we actually trust these results when they’re used to inform planning or policy.

